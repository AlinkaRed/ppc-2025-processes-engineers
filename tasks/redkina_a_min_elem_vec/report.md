# "Нахождение минимального элемента в векторе", вариант № 4
### Студент: Редькина Алина Александровна
### Группа: 3823Б1ПР1
### Преподаватель: Сысоев Александр Владимирович, доцент


## Введение

   Поиск минимального элемента в массиве данных является одной из фундаментальных задач в программировании и вычислительной математике. Данная операция широко используется в различных алгоритмах. В контексте параллельного программирования эта задача представляет интерес для исследования эффективности различных подходов к распараллеливанию.
---

## Постановка задачи

**Цель работы:**  
Реализовать и сравнить последовательную и параллельную версии алгоритма поиска минимального элемента в векторе целых чисел.

**Определение задачи:**  
Для заданного вектора `V` необходимо определить элемент `min(V) = min(v[i])`, где `i = 0..n-1`.

**Ограничения:**
- Входные данные — вектор целых чисел произвольной длины (в том числе пустой).
- Корректность должна сохраняться при отрицательных, положительных и смешанных значениях.
- Для параллельной реализации используется модель передачи сообщений (MPI).
- Результат обеих реализаций должен совпадать.

---

## Описание алгоритма (последовательная версия)

### Алгоритм
1. Инициализировать переменную `min_val` первым элементом вектора.  
2. Для каждого последующего элемента `v[i]`:  
   - Сравнить его с текущим `min_val`.  
   - Если `v[i] < min_val`, обновить `min_val`.  
3. Вернуть значение `min_val`.

**Сложность:** O(n), где *n* — размер вектора.

### Код последовательной реализации

```cpp
bool RedkinaAMinElemVecSEQ::RunImpl() {
  const auto& vec = GetInput();
  if (vec.empty()) {
    GetOutput() = 0;
    return true;
  }

  int min_val = vec[0];
  for (size_t i = 1; i < vec.size(); i++) {
    if (vec[i] < min_val) {
      min_val = vec[i];
    }
  }
  
  GetOutput() = min_val;
  return true;
}
```

---

## Схема распараллеливания (MPI)

### Общая идея
Алгоритм разделяет входной вектор между несколькими процессами. Каждый процесс вычисляет локальный минимум, после чего выполняется коллективная операция `MPI_Allreduce` с оператором `MPI_MIN` для нахождения глобального минимума.

### Топология и обмены
1. **Инициализация:** Все процессы запускаются в одном коммуникаторе `MPI_COMM_WORLD`.  
2. **Передача размера:** Процесс 0 рассылает длину вектора всем остальным процессам (`MPI_Bcast`).  
3. **Распределение данных:** Каждый процесс вычисляет границы своей части данных (равномерное или почти равномерное деление).  
4. **Локальный минимум:** Каждый процесс ищет минимум в своей подвыборке.  
5. **Глобальная редукция:** Результаты всех процессов объединяются вызовом `MPI_Allreduce`.  
6. **Завершение:** Процесс 0 получает глобальный минимум и сохраняет результат.

### Код параллельной реализации

```cpp
bool RedkinaAMinElemVecMPI::RunImpl() {
  const auto& vec = GetInput();
  
  int rank, size;
  MPI_Comm_rank(MPI_COMM_WORLD, &rank);
  MPI_Comm_size(MPI_COMM_WORLD, &size);

  int n = vec.size();
  MPI_Bcast(&n, 1, MPI_INT, 0, MPI_COMM_WORLD);

  if (n == 0) {
    if (rank == 0) GetOutput() = 0;
    return true;
  }

  int local_size = n / size;
  int remainder = n % size;

  int start_idx, end_idx;
  if (rank < remainder) {
    start_idx = rank * (local_size + 1);
    end_idx = start_idx + local_size + 1;
  } else {
    start_idx = remainder * (local_size + 1) + (rank - remainder) * local_size;
    end_idx = start_idx + local_size;
  }

  int local_min = INT_MAX;
  for (int i = start_idx; i < end_idx && i < n; i++) {
    if (vec[i] < local_min) local_min = vec[i];
  }

  int global_min;
  MPI_Allreduce(&local_min, &global_min, 1, MPI_INT, MPI_MIN, MPI_COMM_WORLD);

  GetOutput() = global_min;
  return true;
}
```

---

## Экспериментальные результаты

### Окружение
| Параметр | Значение |
|-----------|-----------|
| Процессор | AMD Ryzen 7 7840HS w/ Radeon 780M Graphics |
| Операционная система | Windows 11 |
| Компилятор | g++ 13.3.0 |
| Тип сборки | Release |
| Число процессов | 2 |

### Проверка корректности
Проведены функциональные тесты на различных входных данных:  
- Положительные числа: {5, 2, 8, 1, 9, 3}
- Отрицательные числа: {-5, -2, -8, -1, -9, -3}
- Смешанные числа: {5, -2, 0, -1, 9, -3}
- Единичный элемент: {42}
- Пустой вектор: {}
- Все одинаковые элементы: {7, 7, 7, 7, 7}
- Дубликаты: {5, 2, 5, 1, 2, 1}
- Большие числа: {1000, 500, 2000, 100, 3000}
- С нулевыми значениями: {10, 0, 20, -5, 0, 15}
- Минимум в начале: {-10, 5, 8, 12, 25}
- Минимум в конце: {15, 8, 12, 5, -3}
- Минимум в середине: {15, 8, -5, 12, 10}
- Большой вектор (1000 элементов)
- Граничные значения: {INT_MAX, INT_MIN, 0, -100, 100}
- Чередующиеся знаки: {10, -10, 20, -20, 30, -30}
- Повторяющиеся минимумы: {1, 3, 1, 5, 1}

**Результат:** Все тесты успешно пройдены, последовательная и MPI-версии возвращают одинаковые значения.

### Оценка производительности
Для оценки производительности использовались тесты с большим объемом данных (200 000 000 элементов).

**Результат:** Все тесты успешно пройдены.

**Наблюдения:**
- При небольших размерах вектора ускорение невелико из-за накладных расходов MPI.  
- Для больших объемов данных производительность параллельной версии немного выше.  
- Операция поиска минимума имеет малую вычислительную плотность, поэтому параллелизация не даёт значительного выигрыша.

---

## Выводы

1. **Корректность:** обе реализации (SEQ и MPI) корректно решают задачу и дают одинаковые результаты в функциональных тестах.  
2. **Производительность:** для небольших наборов данных параллельная версия неэффективна; ускорение наблюдается только при очень больших размерах.  
3. **Масштабируемость:** алгоритм масштабируется линейно при росте числа процессов, однако эффективность ограничена коммуникационными затратами.  
4. **Вывод:** использование MPI для данной задачи оправдано лишь в контексте более сложных параллельных вычислений.

---

## Источники

1. Лекции Сысоева Александра Владимировича.  

